{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab ACtivity 7 - Artificial Neural Network\n",
    "\n",
    "TP noté, à rendre.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "import pandas as pd\n",
    "# Sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "# pip install tensorflow\n",
    "# Télécharger le code source de TensorFlow pour l'utiliser de façon optimale.\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "# Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 15634602 to 15628319\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   Surname          10000 non-null  object \n",
      " 2   CreditScore      10000 non-null  int64  \n",
      " 3   Geography        10000 non-null  object \n",
      " 4   Gender           10000 non-null  object \n",
      " 5   Age              10000 non-null  int64  \n",
      " 6   Tenure           10000 non-null  int64  \n",
      " 7   Balance          10000 non-null  float64\n",
      " 8   NumOfProducts    10000 non-null  int64  \n",
      " 9   HasCrCard        10000 non-null  int64  \n",
      " 10  IsActiveMember   10000 non-null  int64  \n",
      " 11  EstimatedSalary  10000 non-null  float64\n",
      " 12  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import des données, avec comme séparateur \",\" et la colonne d'Index est \"CustomId\".\n",
    "df=pd.read_csv('bank_customers.csv', sep=',').set_index('CustomerId')\n",
    "# Afficher les colonnes et leur type.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspecter et nettoyer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            RowNumber   Surname  CreditScore Geography  Gender  Age  Tenure  \\\n",
      "CustomerId                                                                    \n",
      "15634602            1  Hargrave          619    France  Female   42       2   \n",
      "15647311            2      Hill          608     Spain  Female   41       1   \n",
      "15619304            3      Onio          502    France  Female   42       8   \n",
      "15701354            4      Boni          699    France  Female   39       1   \n",
      "15737888            5  Mitchell          850     Spain  Female   43       2   \n",
      "\n",
      "              Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "CustomerId                                                        \n",
      "15634602         0.00              1          1               1   \n",
      "15647311     83807.86              1          0               1   \n",
      "15619304    159660.80              3          1               0   \n",
      "15701354         0.00              2          0               0   \n",
      "15737888    125510.82              1          1               1   \n",
      "\n",
      "            EstimatedSalary  Exited  \n",
      "CustomerId                           \n",
      "15634602          101348.88       1  \n",
      "15647311          112542.58       0  \n",
      "15619304          113931.57       1  \n",
      "15701354           93826.63       0  \n",
      "15737888           79084.10       0  \n",
      "RowNumber          0\n",
      "Surname            0\n",
      "CreditScore        0\n",
      "Geography          0\n",
      "Gender             0\n",
      "Age                0\n",
      "Tenure             0\n",
      "Balance            0\n",
      "NumOfProducts      0\n",
      "HasCrCard          0\n",
      "IsActiveMember     0\n",
      "EstimatedSalary    0\n",
      "Exited             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des 5 prmières lignes\n",
    "print(df.head())\n",
    "\n",
    "# Combien de valeur manquante par colonne ?\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes inutiles, RowNumber et Surname (qui est un string) :\n",
    "df.drop(['RowNumber', 'Surname'], axis=1, inplace=True)\n",
    "\n",
    "# Gestion des NaN (aucun NaN)\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder de façon catégoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "colonnes_categorielles = ['Gender', 'Geography']\n",
    "df = pd.get_dummies(df, columns=colonnes_categorielles)\n",
    "\n",
    "# Convertir les colonnes résultantes du codage one-hot en int\n",
    "for col in df.columns:\n",
    "    if col.startswith(tuple(colonnes_categorielles)):\n",
    "        df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Diviser les données en entrainement et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X pour les valeurs générales, on prend tout sauf la colonne résultat.\n",
    "# y pour le résultat attendu, on prend seulement la colonne résultat.\n",
    "X = df.drop('Exited', axis=1)  \n",
    "y = df['Exited']\n",
    "\n",
    "# Division, à 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. StandarScaler pour standardiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation sur l'entrainement et le test. \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utiliser keras.models pour initialiser ANN en faisant par couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ajouter la couche d'entrée et la première couche cachée en utilidant la méthode add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\masqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\initializers\\initializers_v1.py:292: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Ajouter la couche d'entrée et la première couche cachée\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ajouter une deuxième couche cachée composée de 6 unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une deuxième couche cachée\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ajouter la couche de sortie avec une unité binaire, et ne pas oublier l'activation sigmoïde fonction à utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la couche de sortie, avec sigmoid.\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configurer le modèle d'apprentissage, via la méthode de comilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\masqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiler le ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ajouster le modèle et appliquer la validation croisée avec une taille de lot de 10. Répéter 50 ou 100 fois (époque = 50 ou 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 1s 89us/sample - loss: 0.3905 - acc: 0.8415\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3899 - acc: 0.8425\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3864 - acc: 0.8418\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 68us/sample - loss: 0.3809 - acc: 0.8431\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 68us/sample - loss: 0.3727 - acc: 0.8444\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 69us/sample - loss: 0.3620 - acc: 0.8549\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3523 - acc: 0.8611\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3472 - acc: 0.8609\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3452 - acc: 0.8619\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3438 - acc: 0.8615\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3429 - acc: 0.8585\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3416 - acc: 0.8624\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3414 - acc: 0.8610\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3406 - acc: 0.8636\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3400 - acc: 0.8620\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 75us/sample - loss: 0.3397 - acc: 0.8615\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3399 - acc: 0.8604\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3392 - acc: 0.8633\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3394 - acc: 0.8614\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3392 - acc: 0.8609\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3384 - acc: 0.8620\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3385 - acc: 0.8626\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3387 - acc: 0.8614\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3378 - acc: 0.8634\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3375 - acc: 0.8615\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3381 - acc: 0.8610\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3371 - acc: 0.8629\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3374 - acc: 0.8614\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3375 - acc: 0.8605\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3370 - acc: 0.8634\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3373 - acc: 0.8608\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3374 - acc: 0.8614\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3369 - acc: 0.8630\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3372 - acc: 0.8620\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3366 - acc: 0.8624\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3374 - acc: 0.8608\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3368 - acc: 0.8622\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3367 - acc: 0.8606\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3370 - acc: 0.8611\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3367 - acc: 0.8626\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3360 - acc: 0.8630\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3366 - acc: 0.8608\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3365 - acc: 0.8612\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3366 - acc: 0.8601\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3364 - acc: 0.8620\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3355 - acc: 0.8602\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3364 - acc: 0.8618\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3356 - acc: 0.8605\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 66us/sample - loss: 0.3363 - acc: 0.8615\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 72us/sample - loss: 0.3356 - acc: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21f1dfcf2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajuster le ANN à l'ensemble d'entraînement\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Faire des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# Prédire les résultats sur l'ensemble de test\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluer le modèle à l'aide de la matrice de confusion et du rapport de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1541   66]\n",
      " [ 216  177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      1607\n",
      "           1       0.73      0.45      0.56       393\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.80      0.70      0.74      2000\n",
      "weighted avg       0.85      0.86      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrice de confusion et rapport de classification\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Prédire si le client avec les informations suivantes quittera la banque ou non : Géographie : France • Pointage de crédit : 600 • Sexe : Homme • Âge : 40 • Ancienneté : 3 • Solde : 60000 • Nombre de produits : 2 • Possède un crédit carte : Oui • Est membre actif : Oui • Salaire estimé : 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Exemple de données client\n",
    "new_customer = [[600, 40, 3, 60000, 2, 1, 1, 50000, 0, 1, 1, 0, 0]]  \n",
    "new_customer = scaler.transform(new_customer)\n",
    "new_prediction = classifier.predict(new_customer)\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "print(new_prediction)  # True pour quitter, False pour rester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Utiliser des modèles Keras séquentiels dans le cadre du flux de trvail Scikit-Learn via le wrappers trouvés dans la bibliothèque Keras. Suivre les instruction données dans le notebook Python pour comprendre comment utiliser KerasClassifier à partir de keras.wrappers.scikit_learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier, batch_size=10, epochs=100)\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10, n_jobs=-1)\n",
    "mean_accuracy = accuracies.mean()\n",
    "variance = accuracies.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision moyenne: 83.81%\n",
      "Variance: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"Précision moyenne: {mean_accuracy*100:.2f}%\")\n",
    "print(f\"Variance: {variance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
